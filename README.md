# DistillNet_Efficient_Image_Classification_via_Knowledge_Distillation
DistillNet is an MLOps-driven project that uses knowledge distillation to compress a large CNN (ResNet50) into a lightweight student model (like ResNet18) for efficient image classification on the CIFAR-10 dataset. It achieves comparable accuracy with significantly reduced model size and inference time.
